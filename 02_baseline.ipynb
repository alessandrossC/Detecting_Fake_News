{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9w5spKiHerg3u+jYb4PH/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alessandrossC/Detecting_Fake_News/blob/main/02_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWVvVue4QHZ_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import xgboost as xgb\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.metrics import log_loss, classification_report, accuracy_score\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_CMk96DQ2i2",
        "outputId": "baafa7cc-ab9c-44f6-f046-c99249fb5cec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = '/content/processed'\n",
        "train_df = pd.read_csv(os.path.join(DATA_PATH, 'train.csv.zip'), compression='zip')\n",
        "val_df = pd.read_csv(os.path.join(DATA_PATH, 'val.csv.zip'), compression='zip')"
      ],
      "metadata": {
        "id": "DrW5hALlQ-dZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DummyClassifier(strategy='most_frequent', random_state=7)\n",
        "model.fit(train_df[['title']], train_df['is_fake'])\n",
        "\n",
        "preds_proba = model.predict_proba(val_df[['title']])\n",
        "preds = model.predict(val_df[['title']])\n",
        "\n",
        "print('Log loss:', log_loss(val_df['is_fake'], preds_proba))\n",
        "print(classification_report(val_df['is_fake'], preds, zero_division=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLdoOShrRVei",
        "outputId": "c94b01af-9f52-4350-ec67-d10bf0820235"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log loss: 17.111786208233823\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00      3394\n",
            "           1       0.53      1.00      0.69      3755\n",
            "\n",
            "    accuracy                           0.53      7149\n",
            "   macro avg       0.26      0.50      0.34      7149\n",
            "weighted avg       0.28      0.53      0.36      7149\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This baseline sets the minimum performance threshold. Any meaningful machine learning model should significantly outperform this result, demonstrating its ability to distinguish between fake and real news."
      ],
      "metadata": {
        "id": "AB0gT7BpSEME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = DummyClassifier(strategy='uniform', random_state=7)\n",
        "model.fit(train_df[['title']], train_df['is_fake'])\n",
        "\n",
        "preds_proba = model.predict_proba(val_df[['title']])\n",
        "preds = model.predict(val_df[['title']])\n",
        "\n",
        "print('Log loss:', log_loss(val_df['is_fake'], preds_proba))\n",
        "print(classification_report(val_df['is_fake'], preds, zero_division=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vp3s7-jlSFU7",
        "outputId": "563cf0a9-8d75-4cde-b328-146d4fc0c538"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log loss: 0.6931471805599454\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.47      0.50      0.49      3394\n",
            "           1       0.52      0.50      0.51      3755\n",
            "\n",
            "    accuracy                           0.50      7149\n",
            "   macro avg       0.50      0.50      0.50      7149\n",
            "weighted avg       0.50      0.50      0.50      7149\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The DummyClassifier with the 'uniform' strategy predicts each class at random, regardless of the input data. As expected, the accuracy, precision, recall, and F1-score for both classes are around 0.50. The log loss is approximately 0.69, which matches the theoretical value for random guessing in a binary classification problem. This result sets a reference point for model performance; any meaningful model should achieve significantly better metrics."
      ],
      "metadata": {
        "id": "TiTdaCMkSmAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_info = joblib.load(os.path.join(DATA_PATH, 'feature_info.joblib'))"
      ],
      "metadata": {
        "id": "KREfDJgaSm96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_features = feature_info['numerical_features']\n",
        "text_features = feature_info['text_features']"
      ],
      "metadata": {
        "id": "HYAB9ByXTB2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_target = train_df['is_fake']\n",
        "val_target = val_df['is_fake']"
      ],
      "metadata": {
        "id": "nLwuIl4eTFc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_vectorizer(\n",
        "        df,\n",
        "        text_col1='title_clean',\n",
        "        text_col2='text_clean',\n",
        "        **vectorizer_params):\n",
        "    \"\"\"\n",
        "    Fit a TfidfVectorizer on two text columns together.\n",
        "    \"\"\"\n",
        "    default_params = {'max_features': 10000, 'ngram_range': (1, 2)}\n",
        "    default_params.update(vectorizer_params)\n",
        "    all_text = pd.concat([df[text_col1], df[text_col2]]).fillna('')\n",
        "    vectorizer = TfidfVectorizer(**default_params)\n",
        "    vectorizer.fit(all_text)\n",
        "    return vectorizer\n",
        "\n",
        "def transform(df, text_col1, text_col2, vectorizer):\n",
        "    \"\"\"\n",
        "    Transform new data using fitted vectorizer.\n",
        "    \"\"\"\n",
        "    vec1 = vectorizer.transform(df[text_col1].fillna(''))\n",
        "    vec2 = vectorizer.transform(df[text_col2].fillna(''))\n",
        "    return vec1, vec2\n",
        "\n",
        "def combine_features(df, numerical_features, vec1, vec2):\n",
        "    \"\"\"\n",
        "    Combine numerical features with vectorized text features.\n",
        "    \"\"\"\n",
        "    return hstack([csr_matrix(df[numerical_features].values), vec1, vec2])"
      ],
      "metadata": {
        "id": "TK4zlQreTP4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = prepare_vectorizer(train_df, text_col1='title_clean', text_col2='text_clean')"
      ],
      "metadata": {
        "id": "nzj0vPrRTkeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_title_vec, train_text_vec = transform(train_df, 'title_clean', 'text_clean', vectorizer)\n",
        "val_title_vec, val_text_vec = transform(val_df, 'title_clean', 'text_clean', vectorizer)"
      ],
      "metadata": {
        "id": "kQEhKjK_UKoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_features = feature_info['numerical_features']\n",
        "X_train = combine_features(train_df, numerical_features, train_title_vec, train_text_vec)\n",
        "X_val = combine_features(val_df, numerical_features, val_title_vec, val_text_vec)"
      ],
      "metadata": {
        "id": "Xioj2bqHURxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = train_df['is_fake']\n",
        "y_val = val_df['is_fake']"
      ],
      "metadata": {
        "id": "mY3TGOMKUhKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_val)\n",
        "print(\"Accuracy:\", accuracy_score(y_val, y_pred))\n",
        "print(classification_report(y_val, y_pred, digits=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COucWl8XWCMW",
        "outputId": "337035fc-d0d4-40a1-8fd1-528295cd4c95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9925863757168835\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.991     0.994     0.992      3394\n",
            "           1      0.994     0.991     0.993      3755\n",
            "\n",
            "    accuracy                          0.993      7149\n",
            "   macro avg      0.992     0.993     0.993      7149\n",
            "weighted avg      0.993     0.993     0.993      7149\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TF-IDF Vectorizer\n",
        "vectorizer = prepare_vectorizer(\n",
        "    train_df,\n",
        "    text_col1='title_clean',\n",
        "    text_col2='text_clean',\n",
        "    max_features=10000\n",
        ")\n",
        "\n",
        "train_title_vec, train_text_vec = transform(\n",
        "    train_df,\n",
        "    text_col1='title_clean',\n",
        "    text_col2='text_clean',\n",
        "    vectorizer=vectorizer\n",
        ")\n",
        "\n",
        "numerical_features = feature_info['numerical_features']\n",
        "\n",
        "X_train = combine_features(\n",
        "    train_df,\n",
        "    numerical_features=numerical_features,\n",
        "    vec1=train_title_vec,\n",
        "    vec2=train_text_vec\n",
        ")\n"
      ],
      "metadata": {
        "id": "9xA30lG1XrbA"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_title_vec, val_text_vec = transform(\n",
        "    val_df,\n",
        "    text_col1='title_clean',\n",
        "    text_col2='text_clean',\n",
        "    vectorizer=vectorizer\n",
        ")\n",
        "\n",
        "X_val = combine_features(\n",
        "    val_df,\n",
        "    numerical_features=numerical_features,\n",
        "    vec1=val_title_vec,\n",
        "    vec2=val_text_vec\n",
        ")"
      ],
      "metadata": {
        "id": "2p-P4bAMXxfO"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import log_loss\n",
        "\n",
        "def evaluate_model(X_train, X_val, y_train, y_val, model):\n",
        "    y_pred_train = model.predict_proba(X_train)\n",
        "    y_pred_val = model.predict_proba(X_val)\n",
        "    loss_train = log_loss(y_train, y_pred_train)\n",
        "    loss_val = log_loss(y_val, y_pred_val)\n",
        "    return loss_train, loss_val\n"
      ],
      "metadata": {
        "id": "Dv-MMpifYySr"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_title_vec, train_text_vec = transform(\n",
        "    train_df,\n",
        "    text_col1='title_clean',\n",
        "    text_col2='text_clean',\n",
        "    vectorizer=vectorizer\n",
        ")\n",
        "\n",
        "train_inputs = combine_features(\n",
        "    train_df,\n",
        "    numerical_features=numerical_features,\n",
        "    vec1=train_title_vec,\n",
        "    vec2=train_text_vec\n",
        ")"
      ],
      "metadata": {
        "id": "7M1Y0nK_ZQMv"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_title_vec, val_text_vec = transform(\n",
        "    val_df,\n",
        "    text_col1='title_clean',\n",
        "    text_col2='text_clean',\n",
        "    vectorizer=vectorizer\n",
        ")\n",
        "\n",
        "val_inputs = combine_features(\n",
        "    val_df,\n",
        "    numerical_features=numerical_features,\n",
        "    vec1=val_title_vec,\n",
        "    vec2=val_text_vec\n",
        ")\n"
      ],
      "metadata": {
        "id": "1of--4x9aV_2"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_lr = LogisticRegression(\n",
        "    random_state=7,\n",
        "    max_iter=1000,\n",
        "    class_weight='balanced',\n",
        "    n_jobs=-1\n",
        ").fit(train_inputs, train_target)\n",
        "\n",
        "log_loss_train, log_loss_val = evaluate_model(\n",
        "    train_inputs, val_inputs, train_target, val_target, model_lr\n",
        ")\n",
        "print('Log loss on Train set:', log_loss_train)\n",
        "print('Log loss on Validation set:', log_loss_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9XA5OBqY725",
        "outputId": "cce7efcb-5572-471f-fcc6-c72ff45ad4b2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log loss on Train set: 0.02731060745113039\n",
            "Log loss on Validation set: 0.031005534227412444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_val_pred = model_lr.predict(val_inputs)\n",
        "print(\"Accuracy:\", accuracy_score(val_target, y_val_pred))\n",
        "print(classification_report(val_target, y_val_pred, digits=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HopHBBxLavJf",
        "outputId": "23356b59-77d4-4f64-d7b4-b86de8830f06"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9921667366065184\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.990     0.993     0.992      3394\n",
            "           1      0.994     0.991     0.993      3755\n",
            "\n",
            "    accuracy                          0.992      7149\n",
            "   macro avg      0.992     0.992     0.992      7149\n",
            "weighted avg      0.992     0.992     0.992      7149\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Logistic Regression model, using a combination of TF-IDF text features and numerical engineered features, achieves extremely high performance on the validation set:\n",
        "\n",
        "Accuracy: 99.2%\n",
        "\n",
        "Precision, Recall, and F1-score: all around 0.99 for both real and fake news classes.\n",
        "\n",
        "This demonstrates the model's exceptional ability to distinguish between fake and real news in this dataset. The results significantly outperform all baselines, confirming that the chosen features and preprocessing steps are highly effective."
      ],
      "metadata": {
        "id": "e6DZOn8Ga8Ia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_results = []\n",
        "results = {}\n",
        "results['model'] = 'Logistic Regression'\n",
        "results['log_loss_train'] = log_loss_train\n",
        "results['log_loss_val'] = log_loss_val\n",
        "\n",
        "experiment_results.append(results)"
      ],
      "metadata": {
        "id": "1wFNNKZMa82T"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_rf = RandomForestClassifier(\n",
        "    n_estimators=10,\n",
        "    class_weight='balanced',\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ").fit(train_inputs, train_target)\n",
        "\n",
        "log_loss_train, log_loss_val = evaluate_model(\n",
        "    train_inputs, val_inputs, train_target, val_target, model_rf\n",
        ")\n",
        "print('Log loss on Train set:', log_loss_train)\n",
        "print('Log loss on Validation set:', log_loss_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-e-LbNLebMqS",
        "outputId": "2ec3c461-4c28-440e-8794-85335e2de780"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log loss on Train set: 0.030701452032678118\n",
            "Log loss on Validation set: 0.08767645515555846\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The RandomForestClassifier achieves a log loss of ~0.03 on the training set and ~0.09 on the validation set.\n",
        "This performance is slightly worse than Logistic Regression on the validation set, indicating that for this specific combination of engineered and text features, Logistic Regression remains the stronger baseline. Further improvements may be possible with boosting algorithms such as XGBoost or LightGBM, or by exploring more advanced neural architectures."
      ],
      "metadata": {
        "id": "zyV-LbvUbkHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_rf = RandomForestClassifier(\n",
        "    n_estimators=10,\n",
        "    max_depth=10,\n",
        "    class_weight='balanced',\n",
        "    n_jobs=-1\n",
        ").fit(train_inputs, train_target)\n",
        "\n",
        "log_loss_train, log_loss_val = evaluate_model(\n",
        "    train_inputs, val_inputs, train_target, val_target, model_rf\n",
        ")\n",
        "print('Log loss on Train set:', log_loss_train)\n",
        "print('Log loss on Validation set:', log_loss_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-X4IxffbkzO",
        "outputId": "ec61b6c4-74dc-4de6-b045-319f51790ccf"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log loss on Train set: 0.2268905329434506\n",
            "Log loss on Validation set: 0.23625998777838195\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_rf = RandomForestClassifier(\n",
        "    n_estimators=10,\n",
        "    max_depth=20,\n",
        "    class_weight='balanced',\n",
        "    n_jobs=-1\n",
        ").fit(train_inputs, train_target)\n",
        "\n",
        "log_loss_train, log_loss_val = evaluate_model(\n",
        "    train_inputs, val_inputs, train_target, val_target, model_rf\n",
        ")\n",
        "print('Log loss on Train set:', log_loss_train)\n",
        "print('Log loss on Validation set:', log_loss_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dwww7A63b3Jy",
        "outputId": "2cdc1a3f-d26b-44cf-fda3-cda41a715ea0"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log loss on Train set: 0.12272193558554284\n",
            "Log loss on Validation set: 0.1551066919834665\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_rf = RandomForestClassifier(\n",
        "    n_estimators=10,\n",
        "    max_leaf_nodes=1000,\n",
        "    class_weight='balanced',\n",
        "    n_jobs=-1\n",
        ").fit(train_inputs, train_target)\n",
        "\n",
        "log_loss_train, log_loss_val = evaluate_model(\n",
        "    train_inputs, val_inputs, train_target, val_target, model_rf\n",
        ")\n",
        "print('Log loss on Train set:', log_loss_train)\n",
        "print('Log loss on Validation set:', log_loss_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zz8pz7EOb9Dq",
        "outputId": "35eaf654-55a5-47dd-a342-dba595441e19"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log loss on Train set: 0.039345427124300385\n",
            "Log loss on Validation set: 0.0828805959466344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_rf = RandomForestClassifier(\n",
        "    n_estimators=10,\n",
        "    max_leaf_nodes=1500,\n",
        "    class_weight='balanced',\n",
        "    n_jobs=-1\n",
        ").fit(train_inputs, train_target)\n",
        "\n",
        "log_loss_train, log_loss_val = evaluate_model(\n",
        "    train_inputs, val_inputs, train_target, val_target, model_rf\n",
        ")\n",
        "print('Log loss on Train set:', log_loss_train)\n",
        "print('Log loss on Validation set:', log_loss_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrwDwCOScLpP",
        "outputId": "0c50e308-cf6d-4263-ecd6-a4c4d38be1f5"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log loss on Train set: 0.02953446240340558\n",
            "Log loss on Validation set: 0.08248911134448124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_rf = RandomForestClassifier(\n",
        "    n_estimators=10,\n",
        "    max_leaf_nodes=1750,\n",
        "    class_weight='balanced',\n",
        "    n_jobs=-1\n",
        ").fit(train_inputs, train_target)\n",
        "\n",
        "log_loss_train, log_loss_val = evaluate_model(\n",
        "    train_inputs, val_inputs, train_target, val_target, model_rf\n",
        ")\n",
        "print('Log loss on Train set:', log_loss_train)\n",
        "print('Log loss on Validation set:', log_loss_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpZe3sLNcT08",
        "outputId": "b12f4318-fee5-43fa-f890-0ac034309449"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log loss on Train set: 0.032718098483617356\n",
            "Log loss on Validation set: 0.09344234582393324\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = {}\n",
        "results['model'] = 'Random Forest'\n",
        "results['log_loss_train'] = log_loss_train\n",
        "results['log_loss_val'] = log_loss_val\n",
        "\n",
        "experiment_results.append(results)"
      ],
      "metadata": {
        "id": "0l5tL7t0cYiQ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(experiment_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "s891bmxEccQi",
        "outputId": "a7cf5ab6-44f3-4287-bb71-33289ca9b72c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 model  log_loss_train  log_loss_val\n",
              "0  Logistic Regression        0.027311      0.031006\n",
              "1        Random Forest        0.032718      0.093442"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4cb6cff4-6671-45f9-b471-4cfe0395b91d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>log_loss_train</th>\n",
              "      <th>log_loss_val</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Logistic Regression</td>\n",
              "      <td>0.027311</td>\n",
              "      <td>0.031006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>0.032718</td>\n",
              "      <td>0.093442</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4cb6cff4-6671-45f9-b471-4cfe0395b91d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4cb6cff4-6671-45f9-b471-4cfe0395b91d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4cb6cff4-6671-45f9-b471-4cfe0395b91d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-56ad9a51-99b7-4358-be8f-e1f7973563a7\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-56ad9a51-99b7-4358-be8f-e1f7973563a7')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-56ad9a51-99b7-4358-be8f-e1f7973563a7 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"model\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Random Forest\",\n          \"Logistic Regression\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"log_loss_train\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.003823673578276978,\n        \"min\": 0.02731060745113039,\n        \"max\": 0.032718098483617356,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.032718098483617356,\n          0.02731060745113039\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"log_loss_val\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04414949287556672,\n        \"min\": 0.031005534227412444,\n        \"max\": 0.09344234582393324,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.09344234582393324,\n          0.031005534227412444\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression should be used as the main baseline for further experiments. Future work could focus on boosting algorithms (like XGBoost or LightGBM) or neural architectures to see if they can further improve upon this strong baseline."
      ],
      "metadata": {
        "id": "6wx6tQYfclX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(experiment_results, os.path.join(DATA_PATH, 'experiment_results.joblib'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRPrdeUMcl9Y",
        "outputId": "6f17f71a-af64-42b5-b6f9-ffe7dc380925"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/processed/experiment_results.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#XGBoost\n",
        "scale_pos_weight = np.round(sum(train_target == 0) / sum(train_target == 1), 4)\n",
        "scale_pos_weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypoepofmcuuS",
        "outputId": "049893de-7fc7-40e7-8157-4650a409a79a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.9038)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_xgb = XGBClassifier(\n",
        "    n_estimators = 10,\n",
        "    learning_rate = 0.1,\n",
        "    random_state = 7,\n",
        "    n_jobs = -1,\n",
        "    eval_metric = 'logloss',\n",
        "    tree_method = 'hist',\n",
        "    scale_pos_weight = scale_pos_weight,\n",
        ").fit(train_inputs, train_target)\n",
        "\n",
        "log_loss_train, log_loss_val = evaluate_model(\n",
        "    train_inputs, val_inputs, train_target, val_target, model_xgb\n",
        ")\n",
        "print('Log loss on Train set:', log_loss_train)\n",
        "print('Log loss on Validation set:', log_loss_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EacGihcUdd2b",
        "outputId": "b6a687b9-1bb5-4922-be8b-467b112008ea"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log loss on Train set: 0.1997330535537069\n",
            "Log loss on Validation set: 0.20010054250054632\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_xgb = XGBClassifier(\n",
        "    n_estimators = 10,\n",
        "    max_depth = 10,\n",
        "    learning_rate = 0.1,\n",
        "    random_state = 7,\n",
        "    n_jobs = -1,\n",
        "    eval_metric = 'logloss',\n",
        "    tree_method = 'hist',\n",
        "    scale_pos_weight = scale_pos_weight,\n",
        ").fit(train_inputs, train_target)\n",
        "\n",
        "log_loss_train, log_loss_val = evaluate_model(\n",
        "    train_inputs, val_inputs, train_target, val_target, model_xgb\n",
        ")\n",
        "print('Log loss on Train set:', log_loss_train)\n",
        "print('Log loss on Validation set:', log_loss_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ve-OPG_eMd_",
        "outputId": "1b162cc3-b718-450d-882b-60e71669d20f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log loss on Train set: 0.1993947482807029\n",
            "Log loss on Validation set: 0.2000494639814533\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best result for Logistic Regression."
      ],
      "metadata": {
        "id": "o3DgtkUUlJJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_match_words(q1, q2):\n",
        "    \"\"\"\n",
        "    Find common words between two text fields (e.g., title and text).\n",
        "    Returns:\n",
        "        match_words: string of common words (space-separated)\n",
        "        count_match_words: number of common words\n",
        "    \"\"\"\n",
        "    try:\n",
        "        q1 = set(str(q1).split())\n",
        "        q2 = set(str(q2).split())\n",
        "    except Exception as e:\n",
        "        q1 = set()\n",
        "        q2 = set()\n",
        "    match_words_list = list(q1 & q2)\n",
        "    count_match_words = len(match_words_list)\n",
        "    match_words = ' '.join(match_words_list) if count_match_words > 0 else ''\n",
        "    return match_words, count_match_words"
      ],
      "metadata": {
        "id": "JZa0QXm5lJzX"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_evaluate_model(\n",
        "    df_train, df_val,\n",
        "    numerical_features, text_col1, text_col2,\n",
        "    vectorizer, model, y_train, y_val\n",
        "):\n",
        "    \"\"\"\n",
        "    Тренує та оцінює модель на числових і текстових фічах одночасно.\n",
        "    \"\"\"\n",
        "    # Fit vectorizer на train\n",
        "    vectorizer.fit(pd.concat([df_train[text_col1], df_train[text_col2]]).fillna(''))\n",
        "\n",
        "    # Трансформуй текст\n",
        "    train_text1_vec = vectorizer.transform(df_train[text_col1].fillna(''))\n",
        "    train_text2_vec = vectorizer.transform(df_train[text_col2].fillna(''))\n",
        "    val_text1_vec = vectorizer.transform(df_val[text_col1].fillna(''))\n",
        "    val_text2_vec = vectorizer.transform(df_val[text_col2].fillna(''))\n",
        "\n",
        "    # Комбінуй із числовими ознаками\n",
        "    X_train = hstack([csr_matrix(df_train[numerical_features].values), train_text1_vec, train_text2_vec])\n",
        "    X_val = hstack([csr_matrix(df_val[numerical_features].values), val_text1_vec, val_text2_vec])\n",
        "\n",
        "    # Train\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Log loss\n",
        "    log_loss_train, log_loss_val = evaluate_model(X_train, X_val, y_train, y_val, model)\n",
        "\n",
        "    return log_loss_train, log_loss_val\n"
      ],
      "metadata": {
        "id": "S7tO3Ao1ll5p"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = pd.DataFrame()\n",
        "X_val = pd.DataFrame()\n",
        "\n",
        "X_train['match_words'] = train_df.apply(\n",
        "    lambda x: get_match_words(x['title_clean'], x['text_clean'])[0], axis=1)\n",
        "X_val['match_words']  = val_df.apply(\n",
        "    lambda x: get_match_words(x['title_clean'], x['text_clean'])[0], axis=1)\n",
        "\n",
        "y_train = train_df['is_fake']\n",
        "y_val = val_df['is_fake']"
      ],
      "metadata": {
        "id": "eDQkNVa0lzYe"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(\n",
        "    stop_words='english',\n",
        "    lowercase=True,\n",
        "    analyzer='word',\n",
        "    max_features=10000\n",
        ")\n",
        "model = LogisticRegression(\n",
        "    max_iter=1000,\n",
        "    random_state=7,\n",
        "    class_weight='balanced',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "log_loss_train, log_loss_val = train_evaluate_model(\n",
        "    train_df, val_df,\n",
        "    numerical_features, 'title_clean', 'text_clean',\n",
        "    vectorizer, model, y_train, y_val\n",
        ")\n",
        "\n",
        "print('Log loss on Train set:', log_loss_train)\n",
        "print('Log loss on Validation set:', log_loss_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yU3ZwewmAFh",
        "outputId": "f021967d-8f12-4bc4-a0a5-915547255471"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log loss on Train set: 0.02765740580529381\n",
            "Log loss on Validation set: 0.03104545215264795\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using only the \"match_words\" feature (the intersection of words between the title and text), the model achieved a log loss of ~0.03 on both the training and validation sets.\n",
        "This result matches the performance of the full feature set, suggesting that this single feature is highly predictive in the current dataset."
      ],
      "metadata": {
        "id": "x7xjG9_Umwzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_results = joblib.load(os.path.join(DATA_PATH, 'experiment_results.joblib'))\n",
        "results = {}\n",
        "results['model'] = 'Logistic Regressin with TF-IDF on matching words'\n",
        "results['log_loss_train'] = np.round(log_loss_train, 5)\n",
        "results['log_loss_val'] = np.round(log_loss_val, 5)\n",
        "\n",
        "experiment_results.append(results)\n",
        "\n",
        "joblib.dump(experiment_results, os.path.join(DATA_PATH, 'experiment_results.joblib'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oA_Q5JXkmwXa",
        "outputId": "9707a124-0507-4467-8bd7-5601437e44c1"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/processed/experiment_results.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    }
  ]
}